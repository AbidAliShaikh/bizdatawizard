[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "GitSetup.html",
    "href": "GitSetup.html",
    "title": "Git Quotes",
    "section": "",
    "text": "Git Command-line Methods\n\n\n1st Approach2nd Approach3rd Approach\n\n\ngit clone <online-repo-url>\n\nIf made any change locally then run:\ngit add -A\ngit commit -m “comt”\ngit push\n\n\n\n\nfrom local to remote\ngit init <directory-name>\ngit add -A\ngit commit -m “msg”\nssh-keygen -t rsa -b 1024 -C “Email@e.com”\nIf file name you give is testkey then locate it and cat testkey.pub\n\ngo to github.com/<repo> ; Click user icon and then select setting\nselect ssh and gpg keys\nselect new ssh key and paste above testkey.pub\n\ngit remote add origin https://github.com/abidalishaikh/<repo>\n\n\n\ngit init\ngit add -A\ngit commit -m “message”\ngit remote add origin https://github.com/abidalishaikh/<repo>\ngit push -u origin master"
  },
  {
    "objectID": "GreenMetric.html",
    "href": "GreenMetric.html",
    "title": "GreenMetric",
    "section": "",
    "text": "GreenMetric University of Indonesia (UI) announced during the December 2023, 68 Pakistani Universities were ranked on various Indicators. The plot below shows some notable Universities with values for various indicators."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quarto Website",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n# The R code chunk\n\nThis is my other online website , where i worked on Distill package from R, however Quarto is a new and latest inclusion from the R community with enhanced features.\nThese are useful R websites:\nhttps://r-graph-gallery.com/\nhttps://ggplot2.tidyverse.org/reference/index.html\nhttps://ggplot2-book.org/"
  },
  {
    "objectID": "Regressionanalysis.html",
    "href": "Regressionanalysis.html",
    "title": "Regressionanalysis",
    "section": "",
    "text": "attach(women)\nlm1 <- lm(weight~height)\nfitted(lm1)\n\n       1        2        3        4        5        6        7        8 \n112.5833 116.0333 119.4833 122.9333 126.3833 129.8333 133.2833 136.7333 \n       9       10       11       12       13       14       15 \n140.1833 143.6333 147.0833 150.5333 153.9833 157.4333 160.8833 \n\nweight\n\n [1] 115 117 120 123 126 129 132 135 139 142 146 150 154 159 164\n\nresiduals(lm1)\n\n          1           2           3           4           5           6 \n 2.41666667  0.96666667  0.51666667  0.06666667 -0.38333333 -0.83333333 \n          7           8           9          10          11          12 \n-1.28333333 -1.73333333 -1.18333333 -1.63333333 -1.08333333 -0.53333333 \n         13          14          15 \n 0.01666667  1.56666667  3.11666667 \n\na=fitted(lm1)+residuals(lm1)\na\n\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15 \n115 117 120 123 126 129 132 135 139 142 146 150 154 159 164 \n\nplot(  height,weight)\nabline(lm1); \n\n\n\nsummary(lm1)\n\n\nCall:\nlm(formula = weight ~ height)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7333 -1.1333 -0.3833  0.7417  3.1167 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***\nheight        3.45000    0.09114   37.85 1.09e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.525 on 13 degrees of freedom\nMultiple R-squared:  0.991, Adjusted R-squared:  0.9903 \nF-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14\n\n#The Residual Standard error 1.525 lbs can be thought of as average error in predicting weight from height\n# if a woman has height of 100, its weight can be predicted as:\nnew_height <- data.frame(height = 100)\npredicted_height <- predict(lm1, newdata = new_height)\npredicted_height\n\n       1 \n257.4833 \n\n#it is more appropriate to predict \n\n\n# F Statics ....\nlm2 <- lm(weight ~ height + I(height^2)) #Quadratic regression\nplot(height,weight); \nlines(height, fitted(lm2),lty=2)\n\n\n\nsummary(lm2)\n\n\nCall:\nlm(formula = weight ~ height + I(height^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.50941 -0.29611 -0.00941  0.28615  0.59706 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 261.87818   25.19677  10.393 2.36e-07 ***\nheight       -7.34832    0.77769  -9.449 6.58e-07 ***\nI(height^2)   0.08306    0.00598  13.891 9.32e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3841 on 12 degrees of freedom\nMultiple R-squared:  0.9995,    Adjusted R-squared:  0.9994 \nF-statistic: 1.139e+04 on 2 and 12 DF,  p-value: < 2.2e-16\n\ndiff(fitted(lm2)); diff(fitted(lm1)) \n\n       2        3        4        5        6        7        8        9 \n2.370168 2.536296 2.702424 2.868552 3.034680 3.200808 3.366936 3.533064 \n      10       11       12       13       14       15 \n3.699192 3.865320 4.031448 4.197576 4.363704 4.529832 \n\n\n   2    3    4    5    6    7    8    9   10   11   12   13   14   15 \n3.45 3.45 3.45 3.45 3.45 3.45 3.45 3.45 3.45 3.45 3.45 3.45 3.45 3.45 \n\npredict(lm2, data.frame(height=100))\n\n       1 \n357.6862 \n\n#Now\n# Weight = 261.8 - 7.34 * height + 0.83 height^2    is a better fit\n# weighted sum of predictor also fits under rubrik of linear regression; \n#in general an nth degree polynomial produces n-1 bends\nlm3 <- lm(weight ~ height + I(height^2)+I(height^3)) #cubic regression\nplot(height,weight); \nlines(height, fitted(lm3))\n\n\n\nsummary(lm3)\n\n\nCall:\nlm(formula = weight ~ height + I(height^2) + I(height^3))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40677 -0.17391  0.03091  0.12051  0.42191 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -8.967e+02  2.946e+02  -3.044  0.01116 * \nheight       4.641e+01  1.366e+01   3.399  0.00594 **\nI(height^2) -7.462e-01  2.105e-01  -3.544  0.00460 **\nI(height^3)  4.253e-03  1.079e-03   3.940  0.00231 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2583 on 11 degrees of freedom\nMultiple R-squared:  0.9998,    Adjusted R-squared:  0.9997 \nF-statistic: 1.679e+04 on 3 and 11 DF,  p-value: < 2.2e-16\n\ndiff(fitted(lm3)); diff(fitted(lm2)) \n\n       2        3        4        5        6        7        8        9 \n2.768207 2.781243 2.819795 2.883861 2.973443 3.088541 3.229153 3.395281 \n      10       11       12       13       14       15 \n3.586925 3.804083 4.046757 4.314946 4.608651 4.927871 \n\n\n       2        3        4        5        6        7        8        9 \n2.370168 2.536296 2.702424 2.868552 3.034680 3.200808 3.366936 3.533064 \n      10       11       12       13       14       15 \n3.699192 3.865320 4.031448 4.197576 4.363704 4.529832 \n\npredict(lm3, data.frame(height=100))\n\n       1 \n535.0433 \n\n##########------------------------------------------------------------------###########\nstates = as.data.frame(state.x77[,c('Murder','Population','Illiteracy','Income','Frost')])\ns2 <- states\nlm1 <- lm(Murder ~ . , data=states)\nsummary(lm1)\n\n\nCall:\nlm(formula = Murder ~ ., data = states)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7960 -1.6495 -0.0811  1.4815  7.6210 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 1.235e+00  3.866e+00   0.319   0.7510    \nPopulation  2.237e-04  9.052e-05   2.471   0.0173 *  \nIlliteracy  4.143e+00  8.744e-01   4.738 2.19e-05 ***\nIncome      6.442e-05  6.837e-04   0.094   0.9253    \nFrost       5.813e-04  1.005e-02   0.058   0.9541    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.535 on 45 degrees of freedom\nMultiple R-squared:  0.567, Adjusted R-squared:  0.5285 \nF-statistic: 14.73 on 4 and 45 DF,  p-value: 9.133e-08\n\n###-----------\nlibrary(ggcorrplot)\n\nLoading required package: ggplot2\n\nggcor <- cor(states[,-1])\nggcorrplot(ggcor, hc.order = TRUE, type = \"lower\", lab = TRUE, lab_size = 3, method=\"circle\",colors = c(\"tomato2\", \"white\", \"springgreen3\"), title=\"Correlogram of Illitercy in US\", ggtheme=theme_bw)\n\n\n\n###-----------\nlm1 <- lm(Murder~. + Income:Frost, data=states)\nsummary(lm1)\n\n\nCall:\nlm(formula = Murder ~ . + Income:Frost, data = states)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.6351 -1.1679 -0.2744  1.6419  5.9772 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   2.451e+01  9.150e+00   2.679  0.01036 * \nPopulation    2.714e-04  8.622e-05   3.148  0.00295 **\nIlliteracy    2.344e+00  1.043e+00   2.247  0.02973 * \nIncome       -4.503e-03  1.769e-03  -2.545  0.01450 * \nFrost        -1.867e-01  6.829e-02  -2.733  0.00900 **\nIncome:Frost  3.922e-05  1.417e-05   2.768  0.00822 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.366 on 44 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5893 \nF-statistic: 15.06 on 5 and 44 DF,  p-value: 1.296e-08\n\nplot(lm1)\n\n\n\n\n\n\n\n\n\n\n\n\ncar::outlierTest(lm1) # Bonferroni outlier test\n\nNo Studentized residuals with Bonferroni p < 0.05\nLargest |rstudent|:\n       rstudent unadjusted p-value Bonferroni p\nNevada 2.991931          0.0045772      0.22886\n\ncar::influencePlot(lm1) # Regression Influence\n\n\n\n\n                StudRes       Hat      CookD\nAlaska        1.3519482 0.4526362 0.24725649\nHawaii       -0.3504961 0.4037481 0.01414625\nNevada        2.9919305 0.1580994 0.23728729\nNorth Dakota -2.1814462 0.1245126 0.10392049\n\nsort(residuals(lm1)) # Nevada has highest residual value 7.6\n\n  North Dakota    Connecticut  Massachusetts   Rhode Island      Minnesota \n    -4.6351037     -3.9508817     -3.9321050     -3.8769752     -3.0941730 \n          Iowa      Wisconsin     New Jersey   Pennsylvania      Louisiana \n    -2.5920895     -2.5690196     -2.4454261     -2.4006150     -2.3679373 \n      Nebraska   South Dakota         Oregon          Texas    Mississippi \n    -1.7603972     -1.4014880     -1.1767318     -1.1414346     -1.0590364 \n       Arizona       Oklahoma  West Virginia     Washington       Arkansas \n    -0.9920987     -0.9887473     -0.8078434     -0.8070507     -0.6997168 \n        Hawaii         Kansas  New Hampshire           Ohio          Maine \n    -0.6467861     -0.6016978     -0.5238584     -0.3911226     -0.3718196 \n      New York South Carolina     California           Utah          Idaho \n    -0.1769995      0.1733659      0.2118959      0.3349708      0.8801093 \n      Delaware        Montana        Indiana North Carolina       Colorado \n     0.8827238      0.9677768      1.0284071      1.1221828      1.1465536 \n     Tennessee     New Mexico       Virginia        Alabama       Illinois \n     1.1588588      1.3944265      1.7243861      1.8994800      1.9118831 \n      Kentucky         Alaska       Maryland        Florida        Vermont \n     2.0421557      2.3444490      2.4906459      2.5579925      2.6700388 \n       Wyoming       Missouri        Georgia       Michigan         Nevada \n     2.7599325      2.9219468      3.3624239      3.4473389      5.9772102 \n\nstates <- states[!(row.names(states) %in% c('Nevada','Alaska')),]\nlm1 <- lm(Murder~. , data=states)\nsummary(lm1)\n\n\nCall:\nlm(formula = Murder ~ ., data = states)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.5237 -1.4830 -0.4602  1.3352  4.0738 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.058e+01  4.187e+00   2.528 0.015221 *  \nPopulation   3.105e-04  7.836e-05   3.962 0.000275 ***\nIlliteracy   2.858e+00  8.322e-01   3.435 0.001326 ** \nIncome      -1.580e-03  7.372e-04  -2.143 0.037832 *  \nFrost       -1.163e-02  8.820e-03  -1.319 0.194165    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.099 on 43 degrees of freedom\nMultiple R-squared:  0.7012,    Adjusted R-squared:  0.6734 \nF-statistic: 25.23 on 4 and 43 DF,  p-value: 8.435e-11\n\nplot(lm1)\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(car); \n\nLoading required package: carData\n\nscatterplotMatrix(states)\n\n\n\nqqPlot(lm1,simulate = T) #when simulate=T , 95% confidence envelope is produced using parametric bootstrap\n\n\n\n\nMichigan  Wyoming \n      21       48 \n\navPlots(lm1)\n\n\n\ninfluencePlot(lm1)\n\n\n\n\n             StudRes        Hat      CookD\nCalifornia -1.009826 0.35728000 0.11332113\nHawaii     -1.252901 0.26209552 0.11005411\nMichigan    2.076071 0.05866864 0.04988512\nWyoming     1.887199 0.07068381 0.05113191\n\nscatterplot(Murder ~ Illiteracy  ,data=states,spread=F, lty.smooth=2,pch=19)\n\n\n\n##INDEPENDENCE OF ERRORS (AUTOCORRELATED) -use durbin watson test\ndurbinWatsonTest(lm1)\n\n lag Autocorrelation D-W Statistic p-value\n   1      -0.2480309       2.36422   0.206\n Alternative hypothesis: rho != 0\n\n# the non-sig value 0.24 suggests a lack of autocorrelation, and conversely independence of errors\n# so the model is good. This test is best for time interval data since closer in time data correlates\nlm2 <- lm (Murder ~ . , data=states)\ncrPlots(lm2) # the model is linear; non-linearity in any one plot suggests including log or polynomial components\n\n\n\nncvTest(lm1)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.6485252, Df = 1, p = 0.42064\n\n# the non-sig p = 0.42 suggests that you have met the constant variance assumption\n# the above tests  homoscedasticity ; hence heteroscedasticity assumption is satisfied\nspreadLevelPlot(lm1)\n\n\n\n\n\nSuggested power transformation:  1.335757 \n\n# the random points around the horizontal line of best fit suggest constant error variance\n# if you'd voileted the assumption, you'd expect non-horizontal line\n# the power transformation 1.33 stabilizes the non-constant error variance\n# if you'd got .5 instead of 1.2, then using sqrt(Y) instead of Y might lead to model that satisfied homoscedasticity\n\n# GOLBAL VALIDATION OF LINEAR MODEL ASSUMPTION \nlibrary(gvlma)\ngv <- gvlma(lm1)\nsummary(gv)\n\n\nCall:\nlm(formula = Murder ~ ., data = states)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.5237 -1.4830 -0.4602  1.3352  4.0738 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.058e+01  4.187e+00   2.528 0.015221 *  \nPopulation   3.105e-04  7.836e-05   3.962 0.000275 ***\nIlliteracy   2.858e+00  8.322e-01   3.435 0.001326 ** \nIncome      -1.580e-03  7.372e-04  -2.143 0.037832 *  \nFrost       -1.163e-02  8.820e-03  -1.319 0.194165    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.099 on 43 degrees of freedom\nMultiple R-squared:  0.7012,    Adjusted R-squared:  0.6734 \nF-statistic: 25.23 on 4 and 43 DF,  p-value: 8.435e-11\n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = lm1) \n\n                     Value p-value                Decision\nGlobal Stat        3.12086  0.5378 Assumptions acceptable.\nSkewness           1.43713  0.2306 Assumptions acceptable.\nKurtosis           1.11038  0.2920 Assumptions acceptable.\nLink Function      0.07398  0.7856 Assumptions acceptable.\nHeteroscedasticity 0.49937  0.4798 Assumptions acceptable.\n\n# IF p WERE SIG IN GLOBAL STAT E.G. 0.05 YOU WOULD HAVE TO ASSESS THE DATA USING PREVIOUS METHODS DISCUSSED\n\n# evaluating multicollinearity\nvif (lm1) # for this to satisfy sqrt(vif) > 2\n\nPopulation Illiteracy     Income      Frost \n  1.320117   2.773789   1.766847   2.168384 \n\nsqrt(vif(lm1)) > 2\n\nPopulation Illiteracy     Income      Frost \n     FALSE      FALSE      FALSE      FALSE \n\n# (all false): this suggests multicollinearity isn't a problem with our predictors\n\nlm2 <- lm(Murder ~ . , data = s2)\noutlierTest(lm2)\n\n       rstudent unadjusted p-value Bonferroni p\nNevada 3.542929         0.00095088     0.047544\n\n# the studentized residual should be between -2 and 2 as right hand rule, but its 3.5 and significant\n# outliertest calculates the one largest residual. You must delete it and rerun the test to see others\n#x11()\navPlots(lm2, ask=F, onepage=T, id.method='identify')\n\n\n\n#right click , esc, or click at the plot to change/identify the outliers \n\n#Another way is ...\ninfluencePlot(lm2, id.method='identify',sub='Circle size is proportional to cooks distance')\n\n\n\n\n                StudRes        Hat       CookD\nAlaska        1.7536917 0.43247319 0.448050997\nCalifornia   -0.2761492 0.34087628 0.008052956\nNevada        3.5429286 0.09508977 0.209915743\nRhode Island -2.0001631 0.04562377 0.035858963\n\n#New York, California, Hawaii, and Washington have high leverage; and Nevada, Alaska, and\n#Hawaii are influential observations. Leverage --> unsual combinations of predictors > .2 or .3\n\n#TRANSFORMING PREDICTORS TO A BETTER FIT\nboxTidwell(weight ~ height, data=women)\n\n MLE of lambda Score Statistic (t)  Pr(>|t|)    \n        4.2008              13.067 1.862e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\niterations =  2 \n\n#suggests that lambda 4.2 or 4, using I(height^4) betters fits the model\n\n#As you’ve just seen, one approach to dealing with multicollinearity is to fit a different\n#type of model (ridge regression in this case). If there are outliers and/or influential\n#observations, you could fit a robust regression model rather than an OLS regression.\n#If you’ve violated the normality assumption, you can fit a nonparametric regression\n#model. If there’s significant nonlinearity, you can try a nonlinear regression model. If\n#you’ve violated the assumptions of independence of errors, you can fit a model that\n#specifically takes the error structure into account, such as time-series models or multi-\n # level regression models. Finally, you can turn to generalized linear models to fit a wide\n#range of models in situations where the assumptions of OLS regression don’t hold.\n\nlibrary(MASS)\nstepAIC(lm2, direction = 'backward') # but\n\nStart:  AIC=97.75\nMurder ~ Population + Illiteracy + Income + Frost\n\n             Df Sum of Sq    RSS     AIC\n- Frost       1     0.021 289.19  95.753\n- Income      1     0.057 289.22  95.759\n<none>                    289.17  97.749\n- Population  1    39.238 328.41 102.111\n- Illiteracy  1   144.264 433.43 115.986\n\nStep:  AIC=95.75\nMurder ~ Population + Illiteracy + Income\n\n             Df Sum of Sq    RSS     AIC\n- Income      1     0.057 289.25  93.763\n<none>                    289.19  95.753\n- Population  1    43.658 332.85 100.783\n- Illiteracy  1   236.196 525.38 123.605\n\nStep:  AIC=93.76\nMurder ~ Population + Illiteracy\n\n             Df Sum of Sq    RSS     AIC\n<none>                    289.25  93.763\n- Population  1    48.517 337.76  99.516\n- Illiteracy  1   299.646 588.89 127.311\n\n\n\nCall:\nlm(formula = Murder ~ Population + Illiteracy, data = s2)\n\nCoefficients:\n(Intercept)   Population   Illiteracy  \n  1.6515497    0.0002242    4.0807366  \n\nstepAIC(lm1, direction = 'backward')\n\nStart:  AIC=75.9\nMurder ~ Population + Illiteracy + Income + Frost\n\n             Df Sum of Sq    RSS    AIC\n- Frost       1     7.664 197.09 75.799\n<none>                    189.43 75.895\n- Income      1    20.227 209.66 78.765\n- Illiteracy  1    51.966 241.40 85.531\n- Population  1    69.162 258.59 88.835\n\nStep:  AIC=75.8\nMurder ~ Population + Illiteracy + Income\n\n             Df Sum of Sq    RSS     AIC\n<none>                    197.09  75.799\n- Income      1    16.120 213.21  77.573\n- Population  1    82.944 280.04  90.659\n- Illiteracy  1   149.483 346.58 100.891\n\n\n\nCall:\nlm(formula = Murder ~ Population + Illiteracy + Income, data = states)\n\nCoefficients:\n(Intercept)   Population   Illiteracy       Income  \n  7.5638451    0.0003323    3.5945824   -0.0013803"
  },
  {
    "objectID": "StatisticswithR.html",
    "href": "StatisticswithR.html",
    "title": "Statistics with R",
    "section": "",
    "text": "In statistics, population is represented by Ω (code: 3A9) while ω (code: 3c9) is one sample from population."
  },
  {
    "objectID": "WebScrapping.html",
    "href": "WebScrapping.html",
    "title": "WebScrapping",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "WebScrapping.html#running-code",
    "href": "WebScrapping.html#running-code",
    "title": "WebScrapping",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  }
]